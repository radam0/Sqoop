To convert from lower case to upper case the predefined sets in tr can be used.

$cat greekfile
Output:

WELCOME TO 
GeeksforGeeks
$cat greekfile | tr “[a-z]” “[A-Z]”
Output:

WELCOME TO
GEEKSFORGEEKS
or

$cat geekfile | tr “[:lower:]” “[:upper:]”
Output:

WELCOME TO
GEEKSFORGEEKS


2. The grep filter searches a file for a particular pattern of characters, and displays all lines that contain that pattern. The pattern that is searched in the file is referred to as the regular expression (grep stands for globally search for regular expression and print out).
Syntax:

grep [options] pattern [files]
Options Description
-c : This prints only a count of the lines that match a pattern
-h : Display the matched lines, but do not display the filenames.
-i : Ignores, case for matching
-l : Displays list of a filenames only.
-n : Display the matched lines and their line numbers.
-v : This prints out all the lines that do not matches the pattern
-e exp : Specifies expression with this option. Can use multiple times.
-f file : Takes patterns from file, one per line.
-E : Treats pattern as an extended regular expression (ERE)
-w : Match whole word
-o : Print only the matched parts of a matching line,
 with each such part on a separate output line.
 
 3.The wc (word count) command in Unix/Linux operating systems is used to find out number of newline count, word count, byte and characters count in a files specified by the file arguments. The syntax of wc command as shown below.

# wc [options] filenames

wc -l : Prints the number of lines in a file.
wc -w : prints the number of words in a file.
wc -c : Displays the count of bytes in a file.
wc -m : prints the count of characters from a file.
wc -L : prints only the length of the longest line in a file.

[root@tecmint ~]# wc -l tecmint.txt
12 tecmint.txt

If (null string) property is not included during sqoop import, then NULLs are stored as [blank for integer columns] and [blank for string columns] in HDFS. 2.If the HIVE table on top of HDFS is queried, we would see [NULL for integer column] and [blank for String columns]
If the (--null-string '\N') property is included during sqoop import, then NULLs are stored as ['\N' for both integer and string columns].
If the HIVE table on top of HDFS is queried, we would see [NULL for both integer and string columns not '\N']


https://www.computerhope.com/unix/used.htm


Import and compress the data
------------------------------

sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/oct2017 --username root --password cloudera --query 'select EMP_NO,birth_date,first_name,last_name,gender,hire_date from employees where EMP_NO>114 and $CONDITIONS'  -z --split-by EMP_NO --target-dir /user/training/sqoop_target_import1

Importing to a target path
If we use –query we must definitely mention –split-by column name

sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/oct2017 --username root --password cloudera --query 'select EMP_NO,birth_date,first_name,last_name,gender,hire_date from employees where EMP_NO>114 and $CONDITIONS' --split-by EMP_NO --target-dir /user/training/sqoop_target_import

sqoop eval --connect jdbc:mysql://quickstart.cloudera:3306/oct2017 \
> --username root \
> --password cloudera \
> --query "select count(*) from employees"


@Geoffrey Shelton Okot 
sqoop import --connect "jdbc:sqlserver://xxxxx:1433;database=xxxxxx;username=xxxxxx;password=xxxxx" --table <table> --hive-import --hive-database <DBname> --incremental append --check-column <name> --last-value 3 -m 1

http://www.hadooptechs.com/sqoop/sqoop-incremental-import-mysql-to-hive

https://acadgild.com/blog/sqoop-tutorial-incremental-imports

https://stackoverflow.com/questions/22969342/sqoop-incremental-import

We can use Sqoop incremental import command with “-merge-key” option for updating the records in an already imported Hive table.

sqoop import --connect jdbc:mysql://localhost/test --table emp
--username hive -password hive --incremental lastmodified --merge-key employee_id 
--check-column emp_timestamp
--target-dir /sqoop/empdata/

--incremental lastmodified will import the updated and new records from RDBMS (MySQL) database based on last latest value of emp_timestamp in Hive.

--merge-key employee_id will "flatten" two datasets into one, taking the newest available records for each primary key (employee_id).

Example:

Let’s assume there 500k records are there in Hive table. In incremental load, we got 100k new employee records and 50k records are updated employee records.
Above sqoop command will import 150K records and using Merge tool it will append new records (100k) and update the 50k records based on primary key (employee_id).

https://dzone.com/articles/hive-incremental-update-using-sqoop

https://community.hortonworks.com/questions/23896/is-it-possible-to-do-an-incremental-import-using-s-1.html

https://community.hortonworks.com/articles/23602/sqoop-fetching-lot-of-tables-in-parallel.html


